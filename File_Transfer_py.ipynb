{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "File_Transfer.py",
      "provenance": [],
      "authorship_tag": "ABX9TyMgxNN6ZXYRLt8IwRPuEfmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boom-Ba/Files_Process/blob/main/File_Transfer_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK-qKeQ8g0-w"
      },
      "source": [
        "import os\n",
        "import boto3\n",
        "import json\n",
        "from boto3 import client\n",
        "import s3fs\n",
        "from boto3.s3.transfer import Transferconfig_tb_name\n",
        "import sys\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "from datetime import date, datetime, timezone\n",
        "import psycopg2\n",
        "from pyspark import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.utils import getResolvedOptions\n",
        "\n",
        "\"\"\"\n",
        "import the connect library for psycopg2\n",
        "import the error handling libraries for psycopg2\n",
        "\"\"\"\n",
        "from psycopg2 import OperationalError, errorcodes, errors\n",
        "import psycopg2.extras as extras\n",
        "\n",
        "#env='TST'/'PPD'\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "logger = glueContext.get_logger()\n",
        "\n",
        "# # get Glue job run id\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'env'])\n",
        "job_run_id = args['JOB_RUN_ID']\n",
        "env = args['env']\n",
        "env = env.upper()\n",
        "\n",
        "\"\"\"\n",
        "set log level default ERROR\n",
        "\"\"\"\n",
        "log_level = \"INFO\"\n",
        "if('--log_level' in sys.argv):\n",
        "    args_log = getResolvedOptions(sys.argv, ['log_level'])\n",
        "    log_level = args_log['log_level']\n",
        "\n",
        "class Glue_Job_Example:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.s3 = boto3.client('s3')\n",
        "        self.to_process=[]\n",
        "        self.host_name = self.get_ssm_parameter(f\"/random-{env}/config_tb_name/v1\")\n",
        "        self.database_name = self.get_ssm_parameter(f\"/random-{env}/config_tb_name/v2\")\n",
        "        self.db_user_name = self.get_ssm_parameter(f\"/random-{env}/config_tb_name/v3\")\n",
        "        self.db_password = self.get_ssm_parameter(f\"/random-{env}/config_tb_name/v4\")\n",
        "        self.s3_bucket_name = self.get_ssm_parameter(f'/random-{env}/config_tb_name/v5')\n",
        "        \n",
        "        self.time_milliseconds = str(time.time() * 1000)\n",
        "\n",
        "    def get_ssm_parameter(self, parameter_name):\n",
        "        \"\"\"\n",
        "        get param from param store\n",
        "        \"\"\"\n",
        "        ssm_client = boto3.client(\"ssm\",\"us-east-1\")\n",
        "        try:\n",
        "            parameter = ssm_client.get_parameter(Name=parameter_name,WithDecryption=True)\n",
        "            return parameter['Parameter']['Value']\n",
        "        except ssm_client.exceptions.ParameterNotFound:\n",
        "            # print(logging.error(f\"didn't work for {parameter_name}\")) \n",
        "            error_message = f\"didn't work for {parameter_name}\"\n",
        "            logger.error(error_message)\n",
        "    \n",
        "    \"\"\"\n",
        "    Create data lineage JSON & Insertion \n",
        "    \"\"\"\n",
        "    def get_data_lineage_crumbs(self, file_name):\n",
        "        DESTINATION_PREFIX = 'parent_folder/processed' \n",
        "        archive_file = \"s3://\" + self.s3_bucket_name +\"/\" + DESTINATION_PREFIX + \"/\" + file_name\n",
        "        time_crumb = self.time_milliseconds\n",
        "        job_name = f\"random-{env}-db_name-data-pipeline-glue-job\"\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            data_linaege_crumb = f\"\"\"{{\"crumb_source\": \"{archive_file}\",\"crumb_source_key\": \"NA\",\"crumb_ts\": \"{time_crumb}\",\"job_name\": \"{job_name}\",\"job_run_id\": \"{job_run_id}\"}}\"\"\"\n",
        "            sql1 = \"UPDATE db_name.t1 set data_lineage_crumbs='\" + data_linaege_crumb + \"'\"\n",
        "            logger.info(sql1)\n",
        "            sql2= \"UPDATE db_name.random_table set data_lineage_crumbs='\" + data_linaege_crumb + \"'\"\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(sql1)\n",
        "            insert_counts=cur.rowcount\n",
        "            logger.info('INSERTION DATA_LINEAGE_CRUMBS: t1 ' + str(insert_counts))\n",
        "            cur.execute(sql2)\n",
        "            insert_counts=cur.rowcount\n",
        "            logger.info('INSERTION DATA_LINEAGE_CRUMBS: random_table ' + str(insert_counts))\n",
        "            conn.commit()\n",
        "            cur.close()\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "    \"\"\"\n",
        "    Deduplication logic happens on main\n",
        "    \"\"\"\n",
        "    def delete_old_records_in_main(self):\n",
        "        try:\n",
        "            query1=\"\"\"DELETE FROM db_name.t1 WHERE CAST(date as DATE) in (select CAST(date as DATE) from db_name.t1_stg)\"\"\"\n",
        "            query2=\"\"\"DELETE FROM db_name.random_table WHERE CAST(conversion_date as DATE) in (select CAST(conversion_date as DATE) from db_name.random_table_stg)\"\"\"\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(query1)\n",
        "            delete_counts=cur.rowcount\n",
        "            logger.info('OVERWRITING RECORDS IN MAIN: t1 '+ str(delete_counts))\n",
        "            cur.execute(query2)\n",
        "            delete_counts=cur.rowcount\n",
        "            logger.info('OVERWRITING RECORDS IN MAIN: random_table ' + str(delete_counts))\n",
        "            conn.commit()\n",
        "            cur.close()\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "    def truncate_stg(self):\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            cur = conn.cursor()\n",
        "            sql5 = \"\"\"DELETE FROM db_name.t1_stg\"\"\"\n",
        "            sql6 = \"\"\"DELETE FROM db_name.random_table_stg\"\"\"\n",
        "            cur.execute(sql5)\n",
        "            logger.info('TRUNCATING STG: t1 ' + str(cur.rowcount))\n",
        "            cur.execute(sql6)\n",
        "            trunc_counts= cur.rowcount\n",
        "            logger.info('TRUNCATING STG: random_table ' + str(trunc_counts))\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "            logger.info('Close connection')\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "    def order_files_by_date_ascending(self):\n",
        "        ordered_file_list =sorted(self.to_process, key = lambda x: x.split('_')[-1], reverse=False)\n",
        "        return ordered_file_list\n",
        "\n",
        "    def create_s3_processed(self):\n",
        "        parent_path = self.ftp_directory_path.split(\"/\")[-1]\n",
        "        s3_processed= parent_path +'processed'\n",
        "        return s3_processed\n",
        "\n",
        "    def upload_file(self):\n",
        "        s3 = boto3.client('s3')\n",
        "        result = s3.list_objects(Bucket = self.s3_bucket_name, Prefix='/parent_folder/to_process')\n",
        "        for o in result.get('Contents'):\n",
        "            data = s3.get_object(Bucket=self.s3_bucket_name, Key=o.get('Key'))\n",
        "            contents = data['Body'].read()\n",
        "            logger.info(contents.decode(\"utf-8\"))\n",
        "\n",
        "    #get to_process s3\n",
        "    def get_to_process_file(self):\n",
        "        self.s3 = boto3.client('s3')\n",
        "        for key in self.s3.list_objects(Bucket=self.s3_bucket_name)['Contents']:\n",
        "            if list(key['Key'].split('/')[:2])==['parent_folder', 'to_process']:\n",
        "                self.to_process.append( list(key['Key'].split('/'))[-1])\n",
        "        return self.to_process \n",
        "    \n",
        "    \"\"\"\"\n",
        "    Archive file from source(to_process) to dest(processed) after data load into RPT\n",
        "    \"\"\"\n",
        "    def archive_file(self):\n",
        "        try:\n",
        "            s3_client = boto3.client('s3')\n",
        "            SOURCE_BUCKET = self.s3_bucket_name\n",
        "            SOURCE_PREFIX = 'parent_folder/to_process' \n",
        "            DESTINATION_BUCKET =  self.s3_bucket_name\n",
        "            DESTINATION_PREFIX = 'parent_folder/processed' \n",
        "            # List objects in source directory\n",
        "            bucket_listing = s3_client.list_objects_v2(Bucket=SOURCE_BUCKET,Prefix=f'{SOURCE_PREFIX}/')\n",
        "            for object in bucket_listing['Contents']:\n",
        "                logger.info('\\n Copying from ' + {object['Key']} ,' to ' + DESTINATION_PREFIX + object['Key'][len(SOURCE_PREFIX):])\n",
        "                s3_client.copy_object(\n",
        "                    CopySource = {'Bucket': SOURCE_BUCKET, 'Key': object['Key']},\n",
        "                    Bucket = DESTINATION_BUCKET,\n",
        "                    Key = DESTINATION_PREFIX + object['Key'][len(SOURCE_PREFIX):] # Remove source prefix, add destination prefix\n",
        "                    )\n",
        "            \n",
        "        except Exception as error:\n",
        "            logger.info('Nothing to Archive: ')\n",
        "     \n",
        "    \"\"\"\n",
        "    Clear To_process s3 bucket after file archive\n",
        "    \"\"\"\n",
        "    def clear_to_process_s3(self):\n",
        "        try:\n",
        "            s3_client = boto3.client('s3')\n",
        "            # SOURCE_BUCKET = 'random-ppd-cr-artifacts'\n",
        "            SOURCE_BUCKET = self.s3_bucket_name\n",
        "            SOURCE_PREFIX = 'parent_folder/to_process' \n",
        "            bucket_listing = s3_client.list_objects(Bucket=SOURCE_BUCKET,Prefix=f'{SOURCE_PREFIX}/')\n",
        "            for obj in bucket_listing['Contents']:\n",
        "                s3_client.delete_object(Bucket=SOURCE_BUCKET, Key=obj['Key'])\n",
        "                logger.info('\\n FILE: ' + obj['Key'] + ' HAS BEEN CLEARED...')\n",
        "        except Exception as error:\n",
        "            logger.info('To Process folder has been cleared: Empty Contents') \n",
        "\n",
        "\n",
        "    #read files main\n",
        "    def read_s3to_aurora(self):\n",
        "        #check file type by substring\n",
        "        file_substring1='t1'\n",
        "        file_substring2='random_table'\n",
        "        # Get all files from to_process folder\n",
        "        self.to_process=self.get_to_process_file()\n",
        "        # Process the older files first\n",
        "        self.to_process=self.order_files_by_date_ascending()\n",
        "        logger.info('ORDERED FILE LIST:')\n",
        "        for file in self.to_process:\n",
        "            logger.info(file)\n",
        "            file_name=file\n",
        "            if file_substring1 in file_name:\n",
        "                self.load_csv(file_name)\n",
        "            elif file_substring2 in file_name:\n",
        "                self.load_gz(file_name)\n",
        "            # checking incorrect files\n",
        "            else:\n",
        "                logger.info('INCORRECT FILE NAME')\n",
        "                logger.info(file_name)\n",
        "                continue\n",
        "            self.delete_old_records_in_main()\n",
        "            self.load_from_stg_to_main()\n",
        "            self.get_data_lineage_crumbs(file_name)\n",
        "            self.truncate_stg()\n",
        "    \n",
        "    def load_gz(self, file_name):\n",
        "        substring2='random_table'\n",
        "        s3_path ='s3://' + self.s3_bucket_name + '/parent_folder/to_process/'+file_name\n",
        "        # read csv files in chunk\n",
        "        chunksize = 10 ** 6\n",
        "        for df in pd.read_csv(s3_path, chunksize=chunksize):\n",
        "            # The code to process df_chunk goes here\n",
        "            if substring2 in file_name:      \n",
        "                table_name='db_name.random_table_stg'\n",
        "                df_columns = list(df.columns)\n",
        "                columns = \",\".join(df_columns)\n",
        "                # create VALUES('%s', '%s\",...) one '%s' per column\n",
        "                values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
        "                insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
        "                conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "                cur=conn.cursor()\n",
        "                psycopg2.extras.execute_batch(cur, insert_stmt, df.values)\n",
        "                conn.commit()\n",
        "                cur.close()\n",
        "        \n",
        "    def load_csv(self,file_name):\n",
        "        s3_path ='s3://' + self.s3_bucket_name + '/parent_folder/to_process/'+file_name\n",
        "        substring1='t1'\n",
        "        # read csv files in chunk\n",
        "        chunksize = 10 ** 6\n",
        "        for df in pd.read_csv(s3_path, chunksize=chunksize):\n",
        "            if len(df) > 0:\n",
        "                if substring1 in file_name:\n",
        "                    table_name='db_name.t1_stg'\n",
        "                    df_columns = list(df.columns)\n",
        "                    columns = \",\".join(df_columns)\n",
        "                    # create VALUES('%s', '%s\",...) one '%s' per column\n",
        "                    values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
        "                    insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
        "                    conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "                    cur=conn.cursor()\n",
        "                    psycopg2.extras.execute_batch(cur, insert_stmt, df.values)\n",
        "                    conn.commit()\n",
        "                    cur.close()\n",
        "                    logger.info('LOAD CSV JOB DONE')\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    def load_from_stg_to_main(self):\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            sql4= \"\"\"INSERT INTO db_name.t1 SELECT * from db_name.t1_stg\"\"\"\n",
        "            sql5= \"\"\"INSERT INTO db_name.random_table SELECT * from db_name.random_table_stg\"\"\"\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(sql4)\n",
        "            insert_counts=cur.rowcount\n",
        "            logger.info('INSERTION STAGE-TO-MAIN: t1 ' + str(insert_counts))\n",
        "            cur.execute(sql5)\n",
        "            insert_counts=cur.rowcount\n",
        "            logger.info('INSERTION STAGE-TO-MAIN: random_table ' + str(insert_counts))\n",
        "            conn.commit()\n",
        "            cur.close()\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "    def read_from_aurora(self, query):\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(query)\n",
        "            res= cur.fetchall()\n",
        "            cur.close()\n",
        "            return res\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "    def create_s3_processed(self):\n",
        "        parent_path = self.ftp_directory_path.split(\"/\")[-1]\n",
        "        s3_processed= parent_path +'processed'\n",
        "        return s3_processed\n",
        "            \n",
        "    def write_to_aurora(self, query):\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(query)\n",
        "            conn.commit()\n",
        "            cur.close()\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "    def write_many_to_aurora(self, query, params):\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            cur = conn.cursor()\n",
        "            cur.executemany(query, params)\n",
        "            conn.commit()\n",
        "            cur.close()\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "    def delete_table(self, query):\n",
        "        try:\n",
        "            conn = psycopg2.connect(host=self.host_name , database=self.database_name ,user=self.db_user_name , password=self.db_password )\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(query)\n",
        "            conn.commit()\n",
        "            cur.close()\n",
        "        except Exception as error:\n",
        "            raise error\n",
        "        finally:\n",
        "            if conn is not None:\n",
        "                conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    job = Glue_Job_Example()\n",
        "    #delete duplicates\n",
        "    job.delete_old_records_in_main()\n",
        "    #main function read from s3 to aurora\n",
        "    job.read_s3to_aurora()\n",
        "    #archive files \n",
        "    job.archive_file()\n",
        "    logger.info('ARCHIVE JOB DONE')\n",
        "    #clear s3 bucket after files has been processed\n",
        "    job.clear_to_process_s3()  \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}